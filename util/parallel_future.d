/**A library that implements various high-level parallelism primitives, such as
 * parallel foreach over arbitrary ranges, parallel map and reduce,
 * futures, and a task pool.
 *
 * By:  David Simcha
 *
 * Examples:
 * ---
    // Create a ThreadPool object with the default number of threads.
    pool = new ThreadPool(4);

    // Create some data to work on.
    uint[] numbers = new uint[1_000];

    // Fill in this array in parallel, using a block size of 100.
    // Note:  Be careful when writing to adjacent elements of an arary from
    // different threads, as this can cause word tearing bugs when
    // the elements aren't properly aligned or aren't the machine's native
    // word size.  In this case, though, we're ok.
    foreach(i; pool.parallel( iota(0, numbers.length)) ) {
        numbers[i] = i;
    }

    // Make sure it works.
    foreach(i; 0..numbers.length) {
        assert(numbers[i] == i);
    }

    stderr.writeln("Done creating nums.");

    // Parallel foreach also works on non-random access ranges, albeit
    // less efficiently.
    auto myNumbers = filter!"a % 7 > 0"( iota(0, 1000));
    foreach(num; pool.parallel(myNumbers)) {
        assert(num % 7 > 0 && num < 1000);
    }
    stderr.writeln("Done modulus test.");

    // Use parallel map to calculate the square of each element in numbers,
    // and make sure it's right.
    uint[] squares = pool.map!"a * a"(numbers, 100);
    assert(squares.length == numbers.length);
    foreach(i, number; numbers) {
        assert(squares[i] == number * number);
    }
    stderr.writeln("Done squares.");

    // Sum up the array in parallel with the current thread.
    auto sumFuture = future!( reduce!"a + b" )(numbers);
    pool.put( &sumFuture);

    // Go off and do other stuff while that future executes:
    // Find the sum of squares of numbers.
    ulong sumSquares = 0;
    foreach(elem; numbers) {
        sumSquares += elem * elem;
    }

    // Ask for our result.  If the task pool has not yet started working on
    // this task, values() automatically steals it and executes it in this
    // thread.
    uint mySum = sumFuture.value;
    assert(mySum == 999 * 1000 / 2);

    // We could have also computed this sum in parallel using parallel
    // reduce.
    auto mySumParallel = pool.reduce!"a + b"(numbers);
    assert(mySum == mySumParallel);
    stderr.writeln("Done sums.");

    // Execute a plain old void delegate task.
    auto myTask = Task({
        synchronized writeln("Our lives are parallel...Our lives are parallel.");
    });
    pool.put( &myTask);

    // Parallel foreach loops can also be nested:
    auto nestedOuter = iota('a', cast(char) ('d' + 1));
    auto nestedInner =  iota(0, 5);

    foreach(letter; pool.parallel(nestedOuter, 1)) {
        foreach(number; pool.parallel(nestedInner, 1)) {
            synchronized writeln(cast(char) letter, number);
        }
    }

    // Block until all jobs are finished and then shut down the thread pool.
    pool.waitStop();
 * ---
 *
 *
 * License:
 * Boost Software License - Version 1.0 - August 17th, 2003
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */
module parallel_future;

import core.thread, std.cpuid, std.algorithm, std.range, std.c.stdlib, std.stdio,
    std.contracts, std.functional, std.conv, std.math, core.memory;

/* Atomics code.  The ASM routines were borrowed from Tango on an ad-hoc basis.
 * These snippeds are likely not copyrightable.  Nonetheless,
 * these should be replaces with generic functions once D2 gets a decent
 * standard atomics lib.
 */
void atomicSetUbyte(ref ubyte stuff, ubyte newVal) {
    asm {
        mov EAX, stuff;
        mov DL, newVal;
        lock;
        xchg [EAX], DL;
    }
}

ubyte atomicReadUbyte(ref ubyte stuff) {
    asm {
        mov DL, 0;
        mov AL, 0;
        mov ECX, stuff;
        lock;
        cmpxchg [ECX], DL;
    }
}

bool atomicCasUbyte(ref ubyte stuff, ubyte testVal, ubyte newVal) {
    asm {
        mov DL, newVal;
        mov AL, testVal;
        mov ECX, stuff;
        lock;
        cmpxchg [ECX], DL;
        setz AL;
    }
}

void atomicIncUint(ref uint num) {
    asm {
        mov EAX, num;
        lock;
        inc int ptr [EAX];
        mov EAX, [EAX];
    }
}

//-----------------------------------------------------------------------------


/*--------------------- Generic helper functions, etc.------------------------*/
private template MapType(alias fun, R) {
    alias typeof(unaryFun!(fun)(ElementType!(R).init)) MapType;
}

private template ReduceType(alias fun, R, E) {
    alias typeof(binaryFun!(fun)(E.init, ElementType!(R).init)) ReduceType;
}

private template hasLength(R) {
    enum bool hasLength = __traits(compiles, R.init.length);
}

private template lValueElements(R) {
    enum bool lValueElements = lValueElementsImpl!(R).ret;
}

private template lValueElementsImpl(R) if(isRandomAccessRange!R) {
    private R myRange;
    enum ret = is(typeof(&(myRange[0])));
}

private void sleepMillisecond(long nMilliseconds) {
    Thread.sleep(nMilliseconds * 10_000);
}

/**Copies an object from wherever it currently resides onto the GC heap
 * and returns a pointer to the newly created copy.
 * Useful if an object, such as a Future struct, is returned from a
 * function by value and you need to escape pointers to it to places where
 * they may be around after the current stack frame disappears.
 */
T* toHeap(T)(T object) {
    GC.BlkAttr gcFlags = (typeid(T).flags & 1) ?
                          cast(GC.BlkAttr) 0 :
                          GC.BlkAttr.NO_SCAN;
    T* myPtr = cast(T*) GC.malloc(T.sizeof, gcFlags);
    *myPtr = object;
    return myPtr;
}

//------------------------------------------------------------------------------


/* Various classes of task.  These use manual C-style polymorphism, the kind
 * with lots of structs and pointer casting.  This is because real classes
 * would prevent some of the allocation tricks I'm using and waste space on
 * monitors and vtbls for something that needs to be ultra-efficient.
 */

enum : ubyte {
    NOT_STARTED,
    IN_PROGRESS,
    DONE
}

// The base "class" for all of the other tasks.
private struct AbstractTask {
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask;
    ubyte taskStatus = NOT_STARTED;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    void job() {
        runTask(&this);
    }
}

/**A Task object for simple void delegate tasks.*/
struct Task {
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask = &impl;
    ubyte taskStatus = NOT_STARTED;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    ThreadPool pool;  // For work stealing stuff.
    void delegate() runMe;

    ///
    this(void delegate() runMe) {
        this.runMe = runMe;
    }

    static void impl(void* myTask) {
        Task* myCastedTask = cast(Task*) myTask;
        myCastedTask.runMe();
    }

    /**If the task isn't started yet, steal it and do it in this thread.
     * If it's done, return.  If it's in progress, busy spin until it's done,
     * then return the return value.
     *
     * This function should be used when you expect the
     * task to be available relatively quickly, on a timescale shorter
     * than that of an OS context switch.
     */
    void wait() {
        enforce(pool !is null, "Job not submitted yet.");
        pool.tryStealDelete( cast(AbstractTask*) &this);

        while(atomicReadUbyte(taskStatus) != DONE) {}
    }

    /**If the task isn't started yet, steal it and do it in this thread.
     * If it's done, return.  If it's in progress, poll at 1 millisecond
     * intervals.
     *
     * This function should be used when you expect the
     * task to take a while, as polling at 1 millisecond intervals
     * introduces latency, but results in negligible wasted CPU cycles.
     */
    void longWait() {
        enforce(pool !is null, "Job not submitted yet.");
        pool.tryStealDelete( cast(AbstractTask*) &this);

        while(atomicReadUbyte(taskStatus) != DONE) {
            sleepMillisecond(1);
        }
    }

    bool done() {
        return atomicReadUbyte(taskStatus) == DONE;
    }
}

private struct ParallelForeachTask(R) if(isRandomAccessRange!R && hasLength!R) {
    // Administrative stuff.
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask = &impl;

    // No, this isn't a bug.  It gets set to NOT_STARTED before submission
    // to the pool.  The reason it's this way is to prevent tasks that were
    // never submitted from being waited on.
    ubyte taskStatus = DONE;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    // More specific stuff.
    size_t lowerBound;
    size_t upperBound;
    R myRange;
    int delegate(ref ElementType!R) runMe;

    static void impl(void* myTask) {
        auto myCastedTask = cast(ParallelForeachTask!(R)*) myTask;
        foreach(i; myCastedTask.lowerBound..myCastedTask.upperBound) {

            static if(lValueElements!R) {
                myCastedTask.runMe( myCastedTask.myRange[i]);
            } else {
                auto valToPass = myCastedTask.myRange[i];
                myCastedTask.runMe(valToPass);
            }

        }

        // Allow some memory reclamation.
        myCastedTask.myRange = R.init;
        myCastedTask.runMe = null;
    }

    void wait() {
        // No work stealing here b/c the function that waits on this task
        // wants to recycle it as soon as it finishes.
        while(atomicReadUbyte(taskStatus) != DONE) {
            sleepMillisecond(1);
        }
    }

    bool done() {
        return atomicReadUbyte(taskStatus) == DONE;
    }
}

private struct ParallelForeachTask(R) if(!isRandomAccessRange!R || !hasLength!R) {
    // Administrative stuff.
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask = &impl;

    // No, this isn't a bug.  It gets set to NOT_STARTED before submission
    // to the pool.  The reason it's this way is to prevent tasks that were
    // never submitted from being waited on.
    ubyte taskStatus = DONE;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    // More specific stuff.
    alias ElementType!R E;
    int delegate(ref E) runMe;
    E[] elements;

    static void impl(void* myTask) {
        auto myCastedTask = cast(ParallelForeachTask!(R)*) myTask;
        foreach(element; myCastedTask.elements) {
            myCastedTask.runMe(element);
        }

        // Make memory easier to reclaim.
        myCastedTask.runMe = null;
    }

    void wait() {
        // No work stealing here b/c the function that waits on this task
        // wants to recycle it as soon as it finishes.

        while(!done()) {
            sleepMillisecond(1);
        }
    }

    bool done() {
        return atomicReadUbyte(taskStatus) == DONE;
    }
}

private struct MapTask(alias fun, R) if(isRandomAccessRange!R && hasLength!R) {
    // Administrative stuff.
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask = &impl;

    // No, this isn't a bug.  It gets set to NOT_STARTED before submission
    // to the pool.  The reason it's this way is to prevent tasks that were
    // never submitted from being waited on.
    ubyte taskStatus = DONE;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    // More specific stuff.
    alias unaryFun!fun uFun;
    R range;
    alias ElementType!R E;
    alias typeof(uFun(E.init)) ReturnType;
    ReturnType[] results;
    size_t lowerBound;
    size_t upperBound;

    static void impl(void* myTask) {
        auto myCastedTask = cast(MapTask!(fun, R)*) myTask;

        foreach(i; myCastedTask.lowerBound..myCastedTask.upperBound) {
            myCastedTask.results[i] = uFun(myCastedTask.range[i]);
        }

        // Nullify stuff, make GC's life easier.
        myCastedTask.results = null;
        myCastedTask.range = R.init;
    }

    void wait() {
        // Again, no work stealing.

        while(!done) {
            sleepMillisecond(1);
        }
    }

    bool done() {
        return atomicReadUbyte(taskStatus) == DONE;
    }
}

/**The task object for futures.  This is usually constructed by calling the
 * future() function.
 */
struct Future(alias fun, Args...) {
    AbstractTask* prev;
    AbstractTask* next;
    void function(void*) runTask = &impl;
    ubyte taskStatus = NOT_STARTED;

    /* Kludge:  Some tasks need to re-submit themselves after they finish.
     * In this case, they will set themselves to NOT_STARTED before
     * resubmitting themselves.  Setting this flag to 0 prevents the work
     * stealing loop from setting them to done.*/
    ubyte shouldSetDone = 1;

    ThreadPool pool;  // For work stealing stuff.
    Args args;

    // Return type stuff.
    alias typeof(fun(args)) ReturnVal;
    static if(!is(ReturnVal == void)) {
        ReturnVal returnVal;
    }

    ///
    this(Args args) {
        this.args = args;
    }

    static void impl(void* myTask) {
        Future!(fun, Args)* myCastedTask = cast(Future!(fun, Args)*) myTask;

        static if(is(ReturnVal == void)) {
            fun(myCastedTask.args);
        } else {
            myCastedTask.returnVal = fun(myCastedTask.args);
        }
    }

    /**If the future isn't started yet, steal it and do it in this thread.
     * If it's done, return its return value, if any.  If it's in progress,
     * busy spin until it's done, then return the return value.
     *
     * This function should be used when you expect the result of the
     * future to be available relatively quickly, on a timescale shorter
     * than that of an OS context switch.
     */
    ReturnVal value() {
        enforce(this.pool !is null, "Job not submitted yet.");

        this.pool.tryStealDelete( cast(AbstractTask*) &this);

        while(atomicReadUbyte(this.taskStatus) != DONE) {}

        static if(!is(ReturnVal == void)) {
            return this.returnVal;
        }
    }

    /**If the future isn't started yet, steal it and do it in this thread.
     * If it's done, return its return value, if any.  If it's in progress,
     * poll at 1 millisecond intervals.
     *
     * This function should be used when you expect the result of the
     * future to take a while, as polling at 1 millisecond intervals
     * introduces latency, but results in negligible wasted CPU cycles.
     */
    ReturnVal longWait() {
        enforce(this.pool !is null, "Job not submitted yet.");
        this.pool.tryStealDelete( cast(AbstractTask*) &this);

        while(atomicReadUbyte(this.taskStatus) != DONE) {
            sleepMillisecond(1);
        }


        static if(!is(ReturnVal == void)) {
            return this.returnVal;
        }
    }

    ///
    bool done() {
        return atomicReadUbyte(this.taskStatus) == DONE;
    }
}

/**Creates a future.
 *
 * Examples:
 * ---
 * auto pool = new ThreadPool();
 * uint[] foo = someFunction();
 *
 * // Create a future to sum this array in the background.
 * auto myFuture = future!( reduce!"a + b" )(foo);
 * pool.put(&myFuture);
 *
 * // Do other stuff.
 *
 * // Get value.  Steals the job and executes it in this thread if it
 * // hasn't been started by a worker thread yet.
 * writeln("Sum = ", myFuture.value);
 * ---
 */
template future(alias fun) {
    ///
    Future!(fun, Args) future(Args...)(Args args) {
        alias Future!(fun, Args) RetType;
        return RetType(args);
    }
}

/**Controls whether the thread pool uses mutex locks or spinlocks to
 * control access to its queue. I haven't benchmarked yet to see which is
 * faster.
 */
enum LockType {
    ///
    SPINLOCK,

    ///
    MUTEX
}

/**The thread pool class that is the workhorse of this library.
 */
class ThreadPool {
private:
    Thread[] pool;
    AbstractTask* head;
    AbstractTask* tail;
    ubyte status = RUNNING;  // All operations on this are done atomically.
    immutable bool useSpinLocks;

    // 1 = locked, 0 = unlocked.
    // BUGS:  Doesn't allow for recursive locking by the same thread.
    // This doesn't matter now b/c this can't happen anyhow, but
    // it should eventually be fixed so it doesn't surprise anyone who tries
    // to modify this code later.
    ubyte spinLock;

    enum : ubyte {
        RUNNING,
        FINISHING,
        STOPNOW
    }

    void workLoop() {
        while(atomicReadUbyte(status) != STOPNOW) {
            AbstractTask* task = pop();
            if (task is null) {
                if(atomicReadUbyte(status) == FINISHING) {
                    atomicSetUbyte(status, STOPNOW);
                    return;
                }

                // TODO:  Make this a condition or something instead of
                // busy spinning.
                sleepMillisecond(1);

            } else {
                assert(task.taskStatus == IN_PROGRESS);
                assert(task.next is null);
                assert(task.prev is null);

                task.job();

                if(task.shouldSetDone) {
                    atomicSetUbyte(task.taskStatus, DONE);
                }
            }
        }
    }

    bool deleteItem(AbstractTask* item) {
        if(useSpinLocks) {
            lockSpinLock();
            auto ret = deleteItemNoSync(item);
            unlockSpinLock();
            return ret;
        } else {
            synchronized(this) {
                return deleteItemNoSync(item);
            }
        }
    }

    bool deleteItemNoSync(AbstractTask* item)
    out {
        assert(item.next is null);
        assert(item.prev is null);
    } body {
        if(item.taskStatus != NOT_STARTED) {
            return false;
        }
        item.taskStatus = IN_PROGRESS;

        if(item is head) {
            // Make sure head gets set properly.
            popNoSync();
            return true;;
        }
        if(item is tail) {
            tail = tail.prev;
            if(tail !is null) {
                tail.next = null;
            }
            item.next = null;
            item.prev = null;
            return true;
        }
        if(item.next !is null) {
            assert(item.next.prev is item);  // Check queue consistency.
            item.next.prev = item.prev;
        }
        if(item.prev !is null) {
            assert(item.prev.next is item);  // Check queue consistency.
            item.prev.next = item.next;
        }
        item.next = null;
        item.prev = null;
        return true;
    }

    // Pop a task off the queue.  Should only be called by worker threads.
    AbstractTask* pop() {
        if(useSpinLocks) {
            lockSpinLock();
            auto ret = popNoSync();
            unlockSpinLock();
            return ret;
        } else {
            synchronized(this) {
                return popNoSync();
            }
        }
    }

    AbstractTask* popNoSync()
    out(returned) {
        /* If task.prev and task.next aren't null, then another thread
         * can try to delete this task from the pool after it's
         * alreadly been deleted/popped.
         */
        if(returned !is null) {
            assert(returned.next is null);
            assert(returned.prev is null);
        }
    } body {
        AbstractTask* returned = head;
        if (head !is null) {
            head = head.next;
            returned.prev = null;
            returned.next = null;
            returned.taskStatus = IN_PROGRESS;
        }
        if(head !is null) {
            head.prev = null;
        }
        return returned;
    }

    // Push a task onto the queue.
    void abstractPut(AbstractTask* task) {
        if(useSpinLocks) {
            lockSpinLock();
            abstractPutNoSync(task);
            unlockSpinLock();
        } else {
            synchronized(this) {
                abstractPutNoSync(task);
            }
        }
    }

    void abstractPutNoSync(AbstractTask* task)
    out {
        assert(tail.prev !is tail);
        assert(tail.next is null, text(tail.prev, '\t', tail.next));
        if(tail.prev !is null) {
            assert(tail.prev.next is tail, text(tail.prev, '\t', tail.next));
        }
    } body {
        task.next = null;
        if (head is null) { //Queue is empty.
            head = task;
            tail = task;
            tail.prev = null;
        } else {
            task.prev = tail;
            tail.next = task;
            tail = task;
        }
    }

    // Same as trySteal, but also deletes the task from the queue so the
    // Task object can be recycled.
    bool tryStealDelete(AbstractTask* toSteal) {
        if( !deleteItem(toSteal) ) {
            return false;
        }

        toSteal.job();

        /* shouldSetDone should always be 1 except if the task re-submits
         * itself to the pool and needs to bypass this.*/
        if(toSteal.shouldSetDone == 1) {
            atomicSetUbyte(toSteal.taskStatus, DONE);
        }

        return true;
    }

    size_t defaultBlockSize(size_t rangeLen) {
        if(this.size == 0) {
            return rangeLen;
        }

        immutable size_t twoSize = 2 * this.size;
        return (rangeLen / twoSize) + ((rangeLen % twoSize == 0) ? 0 : 1);
    }

    void lockSpinLock() {
        while(!atomicCasUbyte(spinLock, 0, 1)) {}
    }

    void unlockSpinLock() {
        bool result = atomicCasUbyte(spinLock, 1, 0);
        assert(result);
    }

public:

    /**Default constructor that initializes a ThreadPool with
     * however many cores are on your CPU.*/
    this(LockType lockType = LockType.MUTEX) {
        this(coresPerCPU - 1, lockType);
    }

    /**Allows for custom poolSize.*/
    this(size_t poolSize, LockType lockType = LockType.MUTEX) {
        pool = new Thread[poolSize];
        foreach(ref poolThread; pool) {
            poolThread = new Thread(&workLoop);
            poolThread.start();
        }
        this.useSpinLocks = (lockType == LockType.SPINLOCK);
    }

    /**Kills pool immediately w/o waiting for jobs to finish.  Use only if you
     * have waitied on every job and therefore know there can't possibly be more
     * in queue, or if you speculatively executed a bunch of stuff and realized
     * you don't need those results anymore.
     *
     * Note:  Does not affect jobs that are already executing, only those
     * in queue.
     */
    final void stop() {
        atomicSetUbyte(status, STOPNOW);
    }

    ///Waits for all jobs to finish, then shuts down the pool.
    final void waitStop() {
        finish();
        foreach(t; pool) {
            auto foo = t.join();
        }
    }

    /**Instructs worker threads to stop when the queue becomes empty, but does
     * not block.
     */
    final void finish() {
        atomicCasUbyte(status, RUNNING, FINISHING);
    }

    ///Returns the number of worker threads in the pool.
    final uint size() {
        return pool.length;
    }

    /// Adds a Task to queue.
    void put(T)(T* task) if(is(T == Task)) {
        task.pool = this;
        abstractPut( cast(AbstractTask*) task);
    }

    /// Adds a Future to the queue.
    final void put(alias fun, Args...)(Future!(fun, Args)* task) {
        task.pool = this;
        abstractPut( cast(AbstractTask*) task);
    }


    /**Implements a parallel foreach loop over a range.  blockSize is the
     * number of elements to process in one work unit.
     *
     * Examples:
     * ---
     * auto pool = new ThreadPool();
     *
     * uint[] squares = new uint[1_000];
     * foreach(i; pool.parallel( iota(0, foo.length), 200)) {
     *     // Iterate over foo using work units of size 100.
     *     squares[i] = i * i;
     * }
     * ---
     *
     * Note:  Since breaking from a loop that's being executed in parallel
     * doesn't make much sense, it is considered undefined behavior in this
     * implementation of parallel foreach.
     *
     */
    ParallelForeach!R parallel(R)(R range, size_t blockSize) {
        alias ParallelForeach!R RetType;
        return RetType(this, range, blockSize);
    }

    /**Parallel foreach with default block size.  For ranges that don't have
     * a length, the default is 512 elements.  For ranges that do, the default
     * is whatever number would create exactly twice as many work units as
     * we have worker threads.
     */
    ParallelForeach!R parallel(R)(R range) {
        static if(hasLength!R) {
            // Default block size is such that we would use 2x as many
            // slots as are in this thread pool.
            size_t blockSize = defaultBlockSize(range.length);
            return parallel(range, blockSize);
        } else {
            // Just use a really, really dumb guess if the user is too lazy to
            // specify.
            return parallel(range, 512);
        }
    }

    /**Parallel map.  Unlike std.algorithm.map, this is evaluated eagerly
     * because it wouldn't make sense to evaluate a parallel map lazily.
     *
     * fun is the function to be evaluated, range is the range to evaluate
     * this function on.  range must be a random access range with length
     * for now, though this restriction may be lifted in the future.
     * blockSize is the size of the work unit to submit to the thread pool,
     * in elements.  buf is an optional buffer to store the results in.
     * If none is provided, one is allocated.  If one is provided, it
     * must have the same length as range.
     *
     * Examples:
     * ---
     * auto pool = new ThreadPool();
     *
     * real[] numbers = new real[1_000];
     * foreach(i, ref num; numbers) {
     *     num = i;
     * }
     *
     * // Find the squares of numbers[] using work units of size 100.
     * real[] squares = pool.map!"a * a"(numbers, 100);
     * ---
     */
    // Bug:  Weird behavior trying to do the overload the normal way.
    // Doing it with templating on the type of blockSize b/c that's the
    // only way it compiles.
    MapType!(fun, R)[] map(alias fun, R, I : size_t)(R range, I blockSize,
        MapType!(fun, R)[] buf = null) {
        immutable len = range.length;

        if(buf.length == 0) {
            buf = new MapType!(fun, R)[len];
        }
        enforce(buf.length == len,
            text("Can't use a user supplied buffer that's the wrong size.  ",
            "(Expected  :", len, " Got:  ", buf.length));
        if(blockSize > len) {
            blockSize = len;
        }

        // Handle as a special case:
        if(size == 0) {
            size_t index = 0;
            foreach(elem; range) {
                buf[index++] = unaryFun!(fun)(elem);
            }
            return buf;
        }

        alias MapTask!(fun, R) MTask;
        MTask[] tasks = (cast(MTask*) alloca(this.size * MTask.sizeof * 2))
                        [0..this.size * 2];
        tasks[] = MTask.init;

        size_t curPos;
        void useTask(ref MTask task) {
            task.lowerBound = curPos;
            task.upperBound = min(len, curPos + blockSize);
            task.range = range;
            task.results = buf;
            curPos += blockSize;

            if(useSpinLocks) {
                lockSpinLock();
                atomicSetUbyte(task.taskStatus, NOT_STARTED);
                abstractPutNoSync(cast(AbstractTask*) &task);
                unlockSpinLock();
            } else {
                synchronized(this) {
                    atomicSetUbyte(task.taskStatus, NOT_STARTED);
                    abstractPutNoSync(cast(AbstractTask*) &task);
                }
            }
        }

        ubyte doneSubmitting = 0;

        Task submitNextBatch;

        void submitJobs() {
            // Search for slots, then sleep.
            foreach(ref task; tasks) if(task.done) {
                useTask(task);
                if(curPos >= len) {
                    atomicSetUbyte(doneSubmitting, 1);
                    return;
                }
            }

            // Now that we've submitted all the worker tasks, submit
            // the next submission task.  Synchronizing on the pool
            // to prevent the stealing thread from deleting the job
            // before it's submitted.
            if(useSpinLocks) {
                lockSpinLock();
                atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                unlockSpinLock();
            } else {
                synchronized(this) {
                    atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                    abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                }
            }
        }

        submitNextBatch = Task(&submitJobs);

        // The submitAndSteal mixin relies on the ThreadPool instance
        // being called pool.
        ThreadPool pool = this;

        mixin(submitAndSteal);

        return buf;
    }

    /**Parallel map with default buffer size.*/
    MapType!(fun, R)[] map(alias fun, R)(R range, MapType!(fun, R)[] buf = null) {
        size_t blkSize = defaultBlockSize(range.length);
        alias map!(fun, R, size_t) mapFun;

        return mapFun(range, blkSize, buf);
    }

    /**Parallel reduce.  For now, the range must offer random access and have
     * a length.  In the future, this restriction may be lifted.
     *
     * Note:  Because this operation is being carried out in parallel,
     * fun must be associative.  For notational simplicity, let # be an
     * infix operator representing fun.  Then, (a # b) # c must equal
     * a # (b # c).  This is NOT the same thing as commutativity.  Matrix
     * multiplication, for example, is associative but not commutative.
     *
     * Examples:
     * ---
     * // Find the max of an array in parallel.  Note that this is a toy example
     * // and unless the comparison function was very expensive, it would
     * // almost always be faster to do this in serial.
     *
     * auto pool = new ThreadPool();
     *
     * auto myArr = somethingExpensiveToCompare();
     * auto myMax = pool.reduce!max(myArr);
     * ---
     */
    ReduceType!(fun, R, E)
    reduce(alias fun, R, E)(E startVal, R range, size_t blockSize = 0) {

        if(size == 0) {
            return std.algorithm.reduce!(fun)(startVal, range);
        }

        // Unlike the rest of the functions here, I can't use the Task object
        // recycling trick here because this has to work on non-commutative
        // operations.  After all the tasks are done executing, fun() has to
        // be applied on the results of these to get a final result, but
        // it can't be evaluated out of order.

        immutable len = range.length;
        if(blockSize == 0) {
            blockSize = defaultBlockSize(len);
        }

        if(blockSize > len) {
            blockSize = len;
        }

        immutable size_t nWorkUnits = (len / blockSize) +
            ((len % blockSize == 0) ? 0 : 1);
        assert(nWorkUnits * blockSize >= len);

        alias binaryFun!fun compiledFun;

        static E reduceOnRange
        (E startVal, R range, size_t lowerBound, size_t upperBound) {
            E result = startVal;
            foreach(i; lowerBound..upperBound) {
                result = compiledFun(result, range[i]);
            }
            return result;
        }

        alias Future!(reduceOnRange, E, R, size_t, size_t) RTask;
        RTask[] tasks;

        enum MAX_STACK = 512;
        immutable size_t nBytesNeeded = nWorkUnits * RTask.sizeof;

        if(nBytesNeeded < MAX_STACK) {
            tasks = (cast(RTask*) alloca(nBytesNeeded))[0..nWorkUnits];
            tasks[] = RTask.init;
        } else {
            tasks = new RTask[nWorkUnits];
        }

        size_t curPos = 0;
        void useTask(ref RTask task) {
            task.args[2] = curPos + 1; // lower bound.
            task.args[3] = min(len, curPos + blockSize);  // upper bound.
            task.args[1] = range;  // range
            task.args[0] = range[curPos];  // Start val.

            curPos += blockSize;
            put(&task);
        }

        foreach(ref task; tasks) {
            useTask(task);
        }

        // Try to steal each of these.
        foreach(ref task; tasks) {
            tryStealDelete( cast(AbstractTask*) &task);
        }

        // Now that we've tried to steal every task, they're all either done
        // or in progress.  Wait on all of them.

        // Using the short, busy wait since we already spent a whole bunch
        // of time trying to steal.  TODO:  Benchmark, make sure this is
        // really the best way in real-world use cases.
        E result = startVal;
        foreach(ref task; tasks) {
            while(atomicReadUbyte(task.taskStatus) != DONE) {}
            result = compiledFun(result, task.returnVal);
        }
        return result;
    }

    /**Parallel reduce with the first element of the range as the start value.*/
    ReduceType!(fun, R, ElementType!R)
    reduce(alias fun, R)(R range, size_t blockSize = 0) {
        auto startVal = range.front;
        range.popFront;
        return reduce!(fun, R, ElementType!R)(startVal, range, blockSize);
    }
}

// Where the magic happens.  This mixin causes tasks to be submitted lazily to
// the task pool.  Attempts are then made by the calling thread to steal
// them.
enum submitAndSteal = q{

    // See documentation for AbstractTask.shouldSetDone.
    submitNextBatch.shouldSetDone = 0;

    // Submit first batch from this thread.
    submitJobs();

    if(doneSubmitting) {
        goto DoneSubmitting;  // So sue me.
    }

    while( !atomicReadUbyte(doneSubmitting) ) {
        // Try to steal parallel foreach tasks.
        foreach(ref task; tasks) {
            pool.tryStealDelete( cast(AbstractTask*) &task);
        }

        // All parallel foreach tasks in progress or done unless next
        // submission task started running.  Try to steal the submission task.
        pool.tryStealDelete(cast(AbstractTask*) &submitNextBatch);
    }

DoneSubmitting:
    // Steal one last time, after they're all submitted.
    foreach(ref task; tasks) {
        pool.tryStealDelete( cast(AbstractTask*) &task);
    }


    foreach(ref task; tasks) {
        task.wait();
    }
};

/*------Structs that implement opApply for parallel foreach.------------------*/
template randLen(R) {
    enum randLen = isRandomAccessRange!R && hasLength!R;
}

private struct ParallelForeach(R) {
    ThreadPool pool;
    R range;
    size_t blockSize;
    ubyte doneSubmitting;

    alias ElementType!R E;
    alias ParallelForeachTask!R PTask;

    int opApply(int delegate(ref E arg) dg) {

        // Handle empty thread pool as special case.
        if(pool.size == 0) {
            int res = 0;
            foreach(ref elem; range) {
                res = dg(elem);
            }
            return res;
        }

        PTask[] tasks = (cast(PTask*) alloca(pool.size * PTask.sizeof * 2))
                        [0..pool.size * 2];
        tasks[] = PTask.init;
        Task submitNextBatch;

        static if(randLen!R) {

            immutable size_t len = range.length;
            size_t curPos = 0;

            void useTask(ref PTask task) {
                task.lowerBound = curPos;
                task.upperBound = min(len, curPos + blockSize);
                task.myRange = range;
                task.runMe = dg;
                curPos += blockSize;

                if(pool.useSpinLocks) {
                    pool.lockSpinLock();
                    atomicSetUbyte(task.taskStatus, NOT_STARTED);
                    pool.abstractPutNoSync(cast(AbstractTask*) &task);
                    pool.unlockSpinLock();
                } else {
                    synchronized(pool) {
                        atomicSetUbyte(task.taskStatus, NOT_STARTED);
                        pool.abstractPutNoSync(cast(AbstractTask*) &task);
                    }
                }
            }

            void submitJobs() {
                // Search for slots to recycle.
                foreach(ref task; tasks) if(task.done) {
                    useTask(task);
                    if(curPos >= len) {
                        atomicSetUbyte(doneSubmitting, 1);
                        return;
                    }
                }

                // Now that we've submitted all the worker tasks, submit
                // the next submission task.  Synchronizing on the pool
                // to prevent the stealing thread from deleting the job
                // before it's submitted.
                if(pool.useSpinLocks) {
                    pool.lockSpinLock();
                    atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                    pool.abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                    pool.unlockSpinLock();
                } else {
                    synchronized(pool) {
                        atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                        pool.abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                    }
                }
            }

        } else {

            void useTask(ref PTask task) {
                task.runMe = dg;
                size_t copyIndex = 0;
                if(task.elements.length == 0) {
                    task.elements = new E[blockSize];
                }

                for(; copyIndex < blockSize && !range.empty; copyIndex++) {
                    task.elements[copyIndex] = range.front;
                    range.popFront;
                }

                // We only actually change the array  size on the last task, when the
                // range is empty.
                task.elements = task.elements[0..copyIndex];

                if(pool.useSpinLocks) {
                    pool.lockSpinLock();
                    atomicSetUbyte(task.taskStatus, NOT_STARTED);
                    pool.abstractPutNoSync(cast(AbstractTask*) &task);
                    pool.unlockSpinLock();
                } else {
                    synchronized(pool) {
                        atomicSetUbyte(task.taskStatus, NOT_STARTED);
                        pool.abstractPutNoSync(cast(AbstractTask*) &task);
                    }
                }
            }


            void submitJobs() {
                // Search for slots to recycle.
                foreach(ref task; tasks) if(task.done) {
                    useTask(task);
                    if(range.empty) {
                        atomicSetUbyte(doneSubmitting, 1);
                        return;
                    }
                }

                // Now that we've submitted all the worker tasks, submit
                // the next submission task.  Synchronizing on the pool
                // to prevent the stealing thread from deleting the job
                // before it's submitted.
                if(pool.useSpinLocks) {
                    pool.lockSpinLock();
                    atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                    pool.abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                    pool.unlockSpinLock();
                } else {
                    synchronized(pool) {
                        atomicSetUbyte(submitNextBatch.taskStatus, NOT_STARTED);
                        pool.abstractPutNoSync( cast(AbstractTask*) &submitNextBatch);
                    }
                }
            }

        }
        submitNextBatch = Task(&submitJobs);

        mixin(submitAndSteal);

        return 0;
    }
}

version(unittest) {
    // This was the only way I could get nested maps to work.
    __gshared ThreadPool pool;
}

// These unittests are intended to also function as an example of how to
// use this module.
unittest {
    foreach(poolSize; [0, 4])
    foreach(useSpinLocks; [LockType.MUTEX, LockType.SPINLOCK]) {

        // Create a ThreadPool object with the default number of threads.
        pool = new ThreadPool(poolSize, useSpinLocks);

        // Create some data to work on.
        uint[] numbers = new uint[1_000];

        // Fill in this array in parallel, using a block size of 100.
        // Note:  Be careful when writing to adjacent elements of an arary from
        // different threads, as this can cause word tearing bugs when
        // the elements aren't properly aligned or aren't the machine's native
        // word size.  In this case, though, we're ok.
        foreach(i; pool.parallel( iota(0, numbers.length)) ) {
            numbers[i] = i;
        }

        // Make sure it works.
        foreach(i; 0..numbers.length) {
            assert(numbers[i] == i);
        }

        stderr.writeln("Done creating nums.");

        // Parallel foreach also works on non-random access ranges, albeit
        // less efficiently.
        auto myNumbers = filter!"a % 7 > 0"( iota(0, 1000));
        foreach(num; pool.parallel(myNumbers)) {
            assert(num % 7 > 0 && num < 1000);
        }
        stderr.writeln("Done modulus test.");

        // Use parallel map to calculate the square of each element in numbers,
        // and make sure it's right.
        uint[] squares = pool.map!"a * a"(numbers, 100);
        assert(squares.length == numbers.length);
        foreach(i, number; numbers) {
            assert(squares[i] == number * number);
        }
        stderr.writeln("Done squares.");

        // Sum up the array in parallel with the current thread.
        auto sumFuture = future!( reduce!"a + b" )(numbers);
        pool.put( &sumFuture);

        // Go off and do other stuff while that future executes:
        // Find the sum of squares of numbers.
        ulong sumSquares = 0;
        foreach(elem; numbers) {
            sumSquares += elem * elem;
        }

        // Ask for our result.  If the task pool has not yet started working on
        // this task, values() automatically steals it and executes it in this
        // thread.
        uint mySum = sumFuture.value;
        assert(mySum == 999 * 1000 / 2);

        // We could have also computed this sum in parallel using parallel
        // reduce.
        auto mySumParallel = pool.reduce!"a + b"(numbers);
        assert(mySum == mySumParallel);
        stderr.writeln("Done sums.");

        // Execute a plain old void delegate task.
        auto myTask = Task({
            synchronized writeln("Our lives are parallel...Our lives are parallel.");
        });
        pool.put( &myTask);

        // Parallel foreach loops can also be nested:
        auto nestedOuter = iota('a', cast(char) ('d' + 1));
        auto nestedInner =  iota(0, 5);

        foreach(letter; pool.parallel(nestedOuter, 1)) {
            foreach(number; pool.parallel(nestedInner, 1)) {
                synchronized writeln(cast(char) letter, number);
            }
        }

        // Block until all jobs are finished and then shut down the thread pool.
        pool.waitStop();
    }
}

// These unittests are intended more for actual testing and not so much
// as examples.
unittest {
    foreach(poolSize; [0, 4])
    foreach(useSpinLocks; [LockType.MUTEX, LockType.SPINLOCK]) {
        pool = new ThreadPool(poolSize, useSpinLocks);

        // Make sure work is reasonably balanced among threads.  This test is
        // non-deterministic and is more of a sanity check than something that
        // has an absolute pass/fail.
        uint[void*] nJobsByThread;
        foreach(thread; pool.pool) {
            nJobsByThread[cast(void*) thread] = 0;
        }
        nJobsByThread[ cast(void*) Thread.getThis] = 0;

        foreach(i; pool.parallel( iota(0, 1_000_000), 100 )) {
            atomicIncUint( nJobsByThread[ cast(void*) Thread.getThis() ]);
        }

        stderr.writeln("\nCurrent (stealing) thread is:  ",
            cast(void*) Thread.getThis());
        stderr.writeln("Workload distribution:  ");
        foreach(k, v; nJobsByThread) {
            stderr.writeln(k, '\t', v);
        }

        // Test whether map can be nested.
        real[][] matrix = new real[][](1000, 1000);
        foreach(i; pool.parallel( iota(0, matrix.length) )) {
            foreach(j; pool.parallel( iota(0, matrix[0].length) )) {
                matrix[i][j] = i * j;
            }
        }

        // Get around weird bugs having to do w/ sqrt being an intrinsic:
        static real mySqrt(real num) {
            return sqrt(num);
        }

        static real[] parallelSqrt(real[] nums) {
            return pool.map!mySqrt(nums);
        }

        real[][] sqrtMatrix = new real[][](1000, 1000);// pool.map!parallelSqrt(matrix);

        foreach(i, row; sqrtMatrix) {
            foreach(j, elem; row) {
                real shouldBe = sqrt( cast(real) i * j);
                //assert(approxEqual(shouldBe, elem));
                sqrtMatrix[i][j] = shouldBe;
            }
        }

        Task saySuccess = Task({
            stderr.writeln(
                "Success doing matrix stuff that involves nested pool use.");
        });
        pool.put(&saySuccess);
        saySuccess.longWait();

        // A more thorough test of map, reduce:  Find the sum of the square roots of
        // matrix.

        static real parallelSum(real[] input) {
            return pool.reduce!"a + b"(input);
        }

        auto sumSqrt = pool.reduce!"a + b"(
            pool.map!parallelSum(
                sqrtMatrix
            )
        );

        assert(approxEqual(sumSqrt, 4.437e8));

        pool.stop();
    }
}
void main(){}

